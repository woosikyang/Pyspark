{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woosikyang/Pyspark/blob/master/tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6p1jGeIk1QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For pyspark environment\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.3.4/spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.3.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.4-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5syUX0NaGewi",
        "colab_type": "text"
      },
      "source": [
        "Spark RDD 란?\n",
        "\n",
        "\n",
        "######출처: https://12bme.tistory.com/306 [길은 가면, 뒤에 있다.]\n",
        "\n",
        "- 데이터셋은 키를 기반으로 chunk 단위로 쪼개져 있고 실행 노드로 분산되어 있다. \n",
        "- 각각 덩어리에 적용된 모든 트랜스포메이션을 추적한다. \n",
        "\n",
        "RDD의 내부 작동 원리\n",
        "\n",
        "- 모든 트랜스포메이션은 데이터셋에 대한 액션이 호출되었을때 실행된다. \n",
        "- 액션은 트랜스포메이션의 순서를 역순으로 해서 매핑을 먼저하고 데이터를 필터링, 이로 인해 더 적은 데이터가 리듀서에 전달됨\n",
        "\n",
        "RDD 생성하기\n",
        "\n",
        "- 컬렉션에 대해 parallelize()함수를 수행 / 외부 어딘가 또는 특정 위치에 저장된 파일을 읽을 수 있음\n",
        "\n",
        "- data = sc.parallelize([('Amber', 22), ('Alfred', 23), ('Skye', 4), ('Albert', 12), ('Amber', 9)])\n",
        "- data_from_file = sc.textFile('/User/drabast/Documents/PySpark_Data/VS14MORT.txt.gz', 4)\n",
        "\n",
        "\n",
        "sc.textFile(..., n)의 마지막 파라미터 n은 데이터셋이 나눠진 파티션의 개수를 의미. 경험상으로 한 클러스터에서는 두 개에서 네 개 정도의 파티션으로 데이터셋을 나누는 것이 가장 좋습니다. 스파크는 NTFS, FAT, Mac OS Extended(HFS+) 또는 HDFS, S3, 카산드라 등의 분산 파일시스템과 같은 여러 개의 파일시스템으로부터 데이터를 읽을 수 있다. 데이터셋을 읽고 저장하는 곳에 주의해야 한다. 경로는 특수문자를 포함할 수 없다.  \n",
        "\n",
        "데이터가 어떻게 읽히느냐에 따라서, 데이터를 지니고 있는 객체는 조금씩 다르게 표현된다. 데이터가 읽히는 파일은 MapPartitionsRDD로 표현된다. 컬렉션에 대해 .parallelize() 함수를 돌릴 때처럼 ParallelCollectionRDD를 사용하지 않는다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbD-Iot5GdnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}